# 广义线性模型

*Author: Limzh*

## 一. 构建广义线性模型的三大假设

### 1.1 指数分布族

$$
p(x;\eta) = h(x)\cdot exp(\eta ^T T(x) - a(\eta))
$$

其中， 

1. $p(x;\eta)$是似然函数
2.  $T(x)$是充分统计量，一般来说，充分统计量直接使用数据本身即可. 也即，$T(x) = x$
3. $a(\eta)$是对数分割函数， 本质上是一个归一化常数。
4. $\eta$ 是一个自然参数或为典范参数，虽然称之为参数但是它也可以是一个关于分布参数的一个函数。

当给定 $T, a, b$之后，就可以应用定义式子确定一个用自然参数确定分布参数的特定指数分布族， $\eta = \eta(\theta)， \eta \rarr \theta$

本质上广义线性模型的构建就是把已有分布重新用指数分布族的角度叙述一遍。

### 1.2 三个假设

对于一个广义线性模型，要符合三个假设

1. ​	$y|x;\theta$服从于指数分布族 （建立自然参数和分布参数的联系）
2. ​    给定x, 目的是预测对应这个x的y的期望。因此，这意味着学习假设 $\mathtt {h_\theta(x) = E[y|x]}$ (建立分布参数和输出的函数联系)
3. 自然参数 $\eta = \theta^Tx $ (线性假设，这是一个设计策略)

## 二. 简单线性模型的广义线性模型构建

### 2.1 假设1：最终预测值符合高斯分布

在这里，我们假设 $y|x;\theta \sim  \mathcal N(\mu, \mathtt 1)$, 由于方差不甚重要，故而简便地设为1. 则，
$$
f_X(y|x) = p(y|x;\mu) = \frac{1}{\sqrt{2 \pi}} \cdot exp\mathtt( -\frac{1}{2}(y - \mu)^2 \mathtt)
$$
根据指数分布族的形式，对该表达式做恒等变换，有:
$$
p(y|x;\mu) = \frac{1}{2} \cdot exp\mathtt(-\frac{1}{2} y^2\mathtt) \cdot exp\mathtt( \mu y - \frac{\mu^2}{2} \mathtt)
$$
对比发现，在这里，

1. $\mathtt{T(y) = y}$
2. $\mathtt{\eta = \mu}$
3. $\mathtt{a(\eta) = \frac{\mu^2}{2}}$

### 2.2 假设2: h(x) = E[y|x]

$$
\mathtt{h(x) = E[y|x;\theta] = \mu = \eta}
$$

而这里的连接函数为 $\mathtt { y = h^{-1}(x)}$， 由于 $\mathtt \eta = \theta^TX$ 的设计策略， $\mathtt h$ 也是一个关于自然参数的函数。对$\tt y|x$的概率分布求解期望的函数，就是链接函数的本质。也即，选择链接函数本质上就是在假设$\tt y|x$的概率分布。

### 2.3 假设3：$\mathtt \eta = \theta^TX$

结合假设2的结论，得出
$$
h(x) = \mathtt{ \theta ^T X}
$$
但要注意，假设3除了说明自然参数和X的线性关系之外还暗含两个信息，第一个是：若一个分布有两个及两个以上的参数，则只能有一个参数可以是自然参数，其他参数必须有默认值。第二个是，虽然对于不同的X，自然参数不一样，但他们共享同一套 $\theta$参数。

这就是简单线性模型背后的概率背景，一切都源于广义线性模型的三个假设。

以上的推导过程说明，应用简单线性模型的前提是：已知任务是“任意给定x之后，y服从类似正态分布”的预测任务。这个前提直接满足广义线性模型的第一假设，之后我们可以使用假设三拟合出自然参数值，再使用假设二函数拟合出预测值，而这个值的在简单线性模型的情境下是 $\mathtt {y|x}$ 的数学期望。

## 三. 逻辑回归的广义线性模型构建

### 3.1 假设1： 最终预测值服从伯努利分布

我们假设， $ \tt y|x;p  \sim \tt Bern(p)$, 那么有：
$$
\tt p (y|x;\cal{p} \tt ) = \cal {p} \tt^y(1-\cal{p} \tt )^{1-y}
$$
恒等变换，使之具有指数分布族的结构
$$
\begin{split}

\tt {p(y|x;p)} &= \tt p^y(1-p)^{1-y} \\ &= \tt exp(y \cdot log(p) + (1- y)log(1- p)) \\ &= \tt exp(y \cdot log(\frac{p}{1-p}) - log(1-p) )
\end{split}
$$
故而， 在逻辑回归的背景下

1. $\tt T(y) = y$
2. $\tt \eta = log(\frac{p}{1-p}), p = \frac{1}{1 + e^{-\eta}}$
3.  $\tt a(\eta) =a(h(p)) = - log(1 - p)$
4. $ \tt b(y) = 1$

### 3.2 假设2： $\tt h_{\theta}(x) = E[y|x;\theta]$

$$
\tt h_{\theta}(x) = E[y|x;\theta] = 1 * p + 0 * (1 -p ) = p
$$

根据假设1中所解出的条件，
$$
\tt h_{\theta}(x) = p = \frac{1}{1 + e^{-\eta}}
$$
在这里我们进一步深化了认识，自然参数和分布参数的关联通过假设1构建指数分布族时由分布类型确定。而假设2的函数必定是一个分布参数的函数。（物理意义是分布的期望，与分布参数有关）。假设2通过分布参数把函数的自变量变成了自然参数。

### 3.3 假设3： $\eta = \theta^TX$

$$
\tt h_{\theta}(x) = \frac{1}{1+e^{-\theta^TX}}
$$

## 六. 损失函数：衡量真实值和拟合值的“距离”

第二专题阐述的是利用模型预测值背后的概率意义。但是，不难发现，第二专题的$\mathtt{P(y|x;\theta)}$是一个似然函数！这意味着我们要从样本中学习权重参数 $\theta$. 那么就需要定义一个成本函数（距离函数）来衡量模型的预测值和真实值之间的距离。

### 6.1 损失函数、成本函数和经验风险函数

这个距离函数也就是所谓的`成本函数`或者 `损失函数`（注：严格地说，成本函数和损失函数一般都指模型在一个样本上的损失，而经验风险指整个训练集上的损失。但有的语境下，损失函数和经验风险同义。但不加说明时候，把损失函数和成本函数视作一样的函数。）. 

一个距离函数有许多的定义方式，以欧式距离的平方（被称为平方损失函数或均方误差函数MSE）为例子

在一个样本和多个样本的情形下，预测产生的损失或者风险如下。
$$
\cal L \tt{(y, h_{\theta}(x))} =  \mit J \tt {(y, h_{\theta}(x)) = \frac{1}{2} \cdot (y - h_{\theta}(x))^2}
$$

$$
\cal L \tt{(y, h_{\theta}(x))} =  \mit J \tt {(y, h_{\theta}(x)) =  \frac{1}{2n} \cdot \mathop{\sum}_{i = 1}^{n} (y^i - h_{\theta}(x^i))^2}
$$



### 6.2 期望风险函数

由于风险函数只是衡量模型在训练集上的损失，所拟合出的模型对整个样本空间的损失并不是最小的。为提高模型的泛化能力，引入期望风险函数。本质上就是对损失函数在联合概率分布$P(x, y)$上求数学期望。

然而遗憾的是，我们根本无法获得所有的样本，故而无法获得$P(x,y)$。根据大数定理，我们通过多个不同训练集的预测取算术平均就可以得到期望风险函数的无偏估计量。根据经验风险的定义，多个不同训练集预测取算术平均就是一个大训练集的经验风险。因此，在实践中，我们也不对期望风险函数和经验风险函数做太多区分，但——当经验风险的定义不再是多个样本损失的加和的算数平均时，两者不同。

（目前不引入结构风险函数）

### 6.3 广义线性模型的损失函数选择

广义线性模型可以（但也可以换）使用平方损失函数作为真实值和预测值之间的距离度量，这种损失有若干优点：

1. 是一个凹函数（采用高数定义，下凸为凹），故局部最优为全局最优
2. 梯度随接近最优而下降，当选取固定的学习率的时候也不必过分担心参数会无法拟合。

缺点就是，平方的存在会放大误差，对异常值相当敏感，因此针对异常值有时会引入均方偏差（MBE）来排除异常值的影响。

当总体训练集的经验风险梯度为0时，代表模型已达到最优。

### 6.4 梯度下降算法

直觉上，梯度下降算法就是一个寻找最快下降方向的算法。
$$
\tt {\theta_j := \theta_j - \alpha \cdot \frac{\partial \mit L \tt (y, h_{\theta}(x))}{\partial \theta_j}, \text{for every j in one sample x} }
$$

#### 6.4.1 随机梯度下降

简单地说，就是迭代地对每一个样本更新一次权重参数，直到收敛或者损失函数极小。

#### 6.4.2 批量随机梯度下降

一次性对所有的样本更新权重，更新若干次直到收敛（必能收敛）。
$$
\tt {\theta_j := \theta_j - \mathop{\sum}_{i = 1}^n\alpha \cdot \frac{\partial \mit L \tt (y, h_{\theta}(x))}{\partial \theta_j}, \text{for every j in one sample x} }
$$
在广义线性模型下，以随机梯度下降算法为：
$$
\tt {\theta_j := \theta_j - \alpha \cdot \frac{\partial \mit L \tt (y, h_{\theta}(x))}{\partial \theta_j} = \theta_j - \alpha \cdot (h_{\theta}(x) - y) \cdot x_j}
$$
找到最优的参数之后，广义线性模型的训练就完成啦！

## 七. 拟合优度 $\mathcal R^2$

拟合优度(goodness of fit) 是指回归直线对观测值的拟合程度。度量拟合优度的统计量是可决系数$R^2$. 该值越接近1说明拟合程度越好，反之则越差。拟合优度的统计量决定系数的定义为回归平方和与总平方和之比。
$$
\cal R^2 = \tt {1 - FVU = 1 - \frac{SSR}{SST} = 1 - \frac{\mathop{\sum}_i (y_i - h_i(x))^2}{\mathop{\sum}_i (y_i - \overline{y})^2} = 1 - \frac{\mathop{\sum}_i (y_i - h_i(x))^2/n}{\mathop{\sum}_i (y_i - \overline{y})^2/n}  = 1 - \frac{RMSE}{Var}}
$$
可以通俗地理解为使用均值作为误差基准，看预测误差是否大于或者小于均值基准误差.

SST指的是`总平方和` 它代表了一个基准模型的误差。想象一个模型，无论给什么输入，它的输出值都是给定验证集的平均值（一条平行于x轴的直线）。如果我们的模型的平均误差（loss）比这个模型还要糟糕（大于等于），那么决定系数就会趋向于0甚至于小于0。这说明直线的拟合程度还不如啥都不预测直接取均值。如果接近1就说明分子标识的误差比基准模型小很多，就说明拟合的比较好。

但要特别注意的是，该统计量只可以用于广义线性模型的拟合优度。

